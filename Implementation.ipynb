{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yolov5\n",
      "  Using cached yolov5-7.0.14-py37.py38.py39.py310-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: gitpython>=3.1.30 in /opt/anaconda3/lib/python3.12/site-packages (from yolov5) (3.1.37)\n",
      "Requirement already satisfied: matplotlib>=3.3 in /opt/anaconda3/lib/python3.12/site-packages (from yolov5) (3.8.4)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/anaconda3/lib/python3.12/site-packages (from yolov5) (1.26.4)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from yolov5) (4.10.0.84)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from yolov5) (10.3.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from yolov5) (5.9.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from yolov5) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from yolov5) (2.32.2)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from yolov5) (1.13.1)\n",
      "Collecting thop>=0.1.1 (from yolov5)\n",
      "  Using cached thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: torch>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from yolov5) (2.4.0)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from yolov5) (0.19.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/anaconda3/lib/python3.12/site-packages (from yolov5) (4.66.4)\n",
      "Collecting ultralytics>=8.0.100 (from yolov5)\n",
      "  Using cached ultralytics-8.3.38-py3-none-any.whl.metadata (35 kB)\n",
      "Requirement already satisfied: tensorboard>=2.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from yolov5) (2.17.1)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from yolov5) (2.2.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from yolov5) (0.13.2)\n",
      "Requirement already satisfied: setuptools>=65.5.1 in /opt/anaconda3/lib/python3.12/site-packages (from yolov5) (69.5.1)\n",
      "Collecting fire (from yolov5)\n",
      "  Using cached fire-0.7.0.tar.gz (87 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting boto3>=1.19.1 (from yolov5)\n",
      "  Using cached boto3-1.35.71-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting sahi>=0.11.10 (from yolov5)\n",
      "  Using cached sahi-0.11.19-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting huggingface-hub<0.25.0,>=0.12.0 (from yolov5)\n",
      "  Using cached huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting roboflow>=0.2.29 (from yolov5)\n",
      "  Using cached roboflow-1.1.49-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting botocore<1.36.0,>=1.35.71 (from boto3>=1.19.1->yolov5)\n",
      "  Using cached botocore-1.35.71-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from boto3>=1.19.1->yolov5) (1.0.1)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.19.1->yolov5)\n",
      "  Using cached s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitpython>=3.1.30->yolov5) (4.0.7)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (4.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3->yolov5) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3->yolov5) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3->yolov5) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3->yolov5) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3->yolov5) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3->yolov5) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.1.4->yolov5) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.1.4->yolov5) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.23.0->yolov5) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.23.0->yolov5) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.23.0->yolov5) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.23.0->yolov5) (2024.7.4)\n",
      "Collecting opencv-python-headless==4.10.0.84 (from roboflow>=0.2.29->yolov5)\n",
      "  Using cached opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from roboflow>=0.2.29->yolov5) (0.21.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from roboflow>=0.2.29->yolov5) (1.16.0)\n",
      "Requirement already satisfied: requests-toolbelt in /opt/anaconda3/lib/python3.12/site-packages (from roboflow>=0.2.29->yolov5) (1.0.0)\n",
      "Collecting filetype (from roboflow>=0.2.29->yolov5)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting opencv-python>=4.1.1 (from yolov5)\n",
      "  Using cached opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: shapely>=1.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from sahi>=0.11.10->yolov5) (2.0.6)\n",
      "Collecting pybboxes==0.1.6 (from sahi>=0.11.10->yolov5)\n",
      "  Using cached pybboxes-0.1.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting terminaltables (from sahi>=0.11.10->yolov5)\n",
      "  Using cached terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from sahi>=0.11.10->yolov5) (8.1.7)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.4.1->yolov5) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.4.1->yolov5) (1.66.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.4.1->yolov5) (3.4.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.4.1->yolov5) (3.20.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.4.1->yolov5) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.4.1->yolov5) (3.0.3)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.7.0->yolov5) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.7.0->yolov5) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.7.0->yolov5) (3.1.4)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/anaconda3/lib/python3.12/site-packages (from ultralytics>=8.0.100->yolov5) (9.0.0)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics>=8.0.100->yolov5)\n",
      "  Using cached ultralytics_thop-2.0.12-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: termcolor in /opt/anaconda3/lib/python3.12/site-packages (from fire->yolov5) (2.4.0)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->yolov5) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->yolov5) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch>=1.7.0->yolov5) (1.3.0)\n",
      "Using cached yolov5-7.0.14-py37.py38.py39.py310-none-any.whl (953 kB)\n",
      "Using cached boto3-1.35.71-py3-none-any.whl (139 kB)\n",
      "Using cached huggingface_hub-0.24.7-py3-none-any.whl (417 kB)\n",
      "Using cached roboflow-1.1.49-py3-none-any.whl (80 kB)\n",
      "Downloading opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (54.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sahi-0.11.19-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.3/111.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pybboxes-0.1.6-py3-none-any.whl (24 kB)\n",
      "Downloading opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Downloading ultralytics-8.3.38-py3-none-any.whl (896 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m896.3/896.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.35.71-py3-none-any.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ultralytics_thop-2.0.12-py3-none-any.whl (26 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114248 sha256=1802b334b441aafab02efd3818a23b468c42e4ef75fdb5f83c052d935ee03a7f\n",
      "  Stored in directory: /Users/omkarthorve/Library/Caches/pip/wheels/9e/5b/45/29f72e55d87a29426b04b3cfdf20325c079eb97ab74f59017d\n",
      "Successfully built fire\n",
      "Installing collected packages: filetype, terminaltables, pybboxes, opencv-python-headless, opencv-python, fire, sahi, huggingface-hub, botocore, ultralytics-thop, thop, s3transfer, roboflow, ultralytics, boto3, yolov5\n",
      "  Attempting uninstall: opencv-python\n",
      "    Found existing installation: opencv-python 4.10.0.84\n",
      "    Uninstalling opencv-python-4.10.0.84:\n",
      "      Successfully uninstalled opencv-python-4.10.0.84\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.25.1\n",
      "    Uninstalling huggingface-hub-0.25.1:\n",
      "      Successfully uninstalled huggingface-hub-0.25.1\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.34.69\n",
      "    Uninstalling botocore-1.34.69:\n",
      "      Successfully uninstalled botocore-1.34.69\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.12.3 requires botocore<1.34.70,>=1.34.41, but you have botocore 1.35.71 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.35.71 botocore-1.35.71 filetype-1.2.0 fire-0.7.0 huggingface-hub-0.24.7 opencv-python-4.9.0.80 opencv-python-headless-4.10.0.84 pybboxes-0.1.6 roboflow-1.1.49 s3transfer-0.10.4 sahi-0.11.19 terminaltables-3.1.10 thop-0.1.1.post2209072238 ultralytics-8.3.38 ultralytics-thop-2.0.12 yolov5-7.0.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/omkarthorve/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from yolov5 import YOLOv5  # Requires YOLOv5 installed\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, target_size):\n",
    "    return cv2.resize(image, target_size)\n",
    "\n",
    "def normalize_image(image, method='alexnet'):\n",
    "    if method == 'alexnet':\n",
    "        return image / 255.0  # Scale to [0,1]\n",
    "    elif method == 'yolov5':\n",
    "        return (image / 127.5) - 1.0  # Normalize to [-1, 1]\n",
    "\n",
    "def augment_image(image):\n",
    "    datagen = ImageDataGenerator(rotation_range=30,\n",
    "                                 width_shift_range=0.1,\n",
    "                                 height_shift_range=0.1,\n",
    "                                 zoom_range=0.2,\n",
    "                                 horizontal_flip=True)\n",
    "    return datagen.random_transform(image)\n",
    "\n",
    "def apply_gaussian_blur(image):\n",
    "    return cv2.GaussianBlur(image, (5, 5), 0)\n",
    "\n",
    "def hsv_conversion(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "def mean_subtraction(image, mean_values):\n",
    "    return image - mean_values\n",
    "\n",
    "def normalize_bounding_boxes(boxes, image_dims):\n",
    "    h, w = image_dims\n",
    "    normalized_boxes = boxes.copy()\n",
    "    normalized_boxes[:, 0] /= w\n",
    "    normalized_boxes[:, 1] /= h\n",
    "    normalized_boxes[:, 2] /= w\n",
    "    normalized_boxes[:, 3] /= h\n",
    "    return normalized_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Found no valid file for the classes labels. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m\n\u001b[1;32m      5\u001b[0m data_transforms \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      7\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     ])\n\u001b[1;32m     16\u001b[0m }\n\u001b[1;32m     18\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/omkarthorve/Documents/Ganga Flow/Dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 19\u001b[0m image_datasets \u001b[38;5;241m=\u001b[39m {x: ImageFolder(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, transform\u001b[38;5;241m=\u001b[39mdata_transforms[x])\n\u001b[1;32m     20\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     21\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m {x: DataLoader(image_datasets[x], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m                \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchvision/datasets/folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    321\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    327\u001b[0m ):\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    329\u001b[0m         root,\n\u001b[1;32m    330\u001b[0m         loader,\n\u001b[1;32m    331\u001b[0m         IMG_EXTENSIONS \u001b[38;5;28;01mif\u001b[39;00m is_valid_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    332\u001b[0m         transform\u001b[38;5;241m=\u001b[39mtransform,\n\u001b[1;32m    333\u001b[0m         target_transform\u001b[38;5;241m=\u001b[39mtarget_transform,\n\u001b[1;32m    334\u001b[0m         is_valid_file\u001b[38;5;241m=\u001b[39mis_valid_file,\n\u001b[1;32m    335\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[1;32m    336\u001b[0m     )\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchvision/datasets/folder.py:150\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m    149\u001b[0m classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_classes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n\u001b[0;32m--> 150\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[1;32m    152\u001b[0m     class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[1;32m    153\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    154\u001b[0m     is_valid_file\u001b[38;5;241m=\u001b[39mis_valid_file,\n\u001b[1;32m    155\u001b[0m     allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[1;32m    156\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextensions \u001b[38;5;241m=\u001b[39m extensions\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchvision/datasets/folder.py:203\u001b[0m, in \u001b[0;36mDatasetFolder.make_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_to_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# prevent potential bug since make_dataset() would use the class_to_idx logic of the\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# find_classes() function, instead of using that of the find_classes() method, which\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# is potentially overridden and thus could have a different logic.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe class_to_idx parameter cannot be None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m make_dataset(\n\u001b[1;32m    204\u001b[0m     directory, class_to_idx, extensions\u001b[38;5;241m=\u001b[39mextensions, is_valid_file\u001b[38;5;241m=\u001b[39mis_valid_file, allow_empty\u001b[38;5;241m=\u001b[39mallow_empty\n\u001b[1;32m    205\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchvision/datasets/folder.py:104\u001b[0m, in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m         msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupported extensions are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextensions\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28misinstance\u001b[39m(extensions,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m instances\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Found no valid file for the classes labels. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp"
     ]
    }
   ],
   "source": [
    "# Load dataset (using roboflow dataset path or local directory)\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Mean subtraction\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = '/Users/omkarthorve/Documents/Ganga Flow/Dataset'\n",
    "image_datasets = {x: ImageFolder(root=f\"{data_dir}/{x}\", transform=data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=16, shuffle=True)\n",
    "               for x in ['train', 'val']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root folder for dataset\n",
    "dataset_path = \"/Users/omkarthorve/Documents/Ganga Flow/Dataset\"\n",
    "\n",
    "# Subdirectories\n",
    "train_images_path = os.path.join(dataset_path, \"train/images\")\n",
    "train_labels_path = os.path.join(dataset_path, \"train/labels\")\n",
    "valid_images_path = os.path.join(dataset_path, \"valid/images\")\n",
    "valid_labels_path = os.path.join(dataset_path, \"valid/labels\")\n",
    "test_images_path = os.path.join(dataset_path, \"test/images\")\n",
    "test_labels_path = os.path.join(dataset_path, \"test/labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_labels(image_dir, label_dir, target_size=(224, 224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for img_name in os.listdir(image_dir):\n",
    "        # Load image\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        img = load_img(img_path, target_size=target_size)\n",
    "        img = img_to_array(img)\n",
    "        \n",
    "        # Load corresponding label\n",
    "        label_path = os.path.join(label_dir, os.path.splitext(img_name)[0] + \".txt\")\n",
    "        bounding_boxes = []  # Store bounding boxes and class IDs\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as file:\n",
    "                for line in file.readlines():\n",
    "                    # Parse YOLO-style label line\n",
    "                    values = list(map(float, line.strip().split()))\n",
    "                    class_id = int(values[0])  # First value is the class ID\n",
    "                    bounding_boxes.append(values)  # Append the full bounding box info\n",
    "\n",
    "        images.append(img)\n",
    "        labels.append(bounding_boxes)  # Save bounding box data\n",
    "\n",
    "    images = np.array(images, dtype=\"float32\")\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Normalization and Augmentation\n",
    "def preprocess_images(images, target_range=(0, 1)):\n",
    "    # Rescale pixel values\n",
    "    min_val, max_val = target_range\n",
    "    images = (images / 255.0) * (max_val - min_val) + min_val\n",
    "    \n",
    "    # Augmentation (if needed)\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "    return images, datagen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m test_images, _ \u001b[38;5;241m=\u001b[39m preprocess_images(test_images)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Convert labels to categorical if needed (e.g., for classification tasks)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(train_labels))\n\u001b[1;32m     13\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m to_categorical(train_labels, num_classes)\n\u001b[1;32m     14\u001b[0m valid_labels \u001b[38;5;241m=\u001b[39m to_categorical(valid_labels, num_classes)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# Load training, validation, and test data\n",
    "train_images, train_labels = load_images_and_labels(train_images_path, train_labels_path)\n",
    "valid_images, valid_labels = load_images_and_labels(valid_images_path, valid_labels_path)\n",
    "test_images, test_labels = load_images_and_labels(test_images_path, test_labels_path)\n",
    "\n",
    "# Preprocess data\n",
    "train_images, train_datagen = preprocess_images(train_images)\n",
    "valid_images, _ = preprocess_images(valid_images)\n",
    "test_images, _ = preprocess_images(test_images)\n",
    "\n",
    "# Convert labels to categorical if needed (e.g., for classification tasks)\n",
    "num_classes = len(set(train_labels))\n",
    "train_labels = to_categorical(train_labels, num_classes)\n",
    "valid_labels = to_categorical(valid_labels, num_classes)\n",
    "test_labels = to_categorical(test_labels, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(train_labels))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Convert labels to categorical for classification\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m to_categorical(train_labels, num_classes)\n\u001b[1;32m     16\u001b[0m valid_labels \u001b[38;5;241m=\u001b[39m to_categorical(valid_labels, num_classes)\n\u001b[1;32m     17\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m to_categorical(test_labels, num_classes)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/numerical_utils.py:98\u001b[0m, in \u001b[0;36mto_categorical\u001b[0;34m(x, num_classes)\u001b[0m\n\u001b[1;32m     96\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     97\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size, num_classes))\n\u001b[0;32m---> 98\u001b[0m categorical[np\u001b[38;5;241m.\u001b[39marange(batch_size), x] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     99\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m input_shape \u001b[38;5;241m+\u001b[39m (num_classes,)\n\u001b[1;32m    100\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(categorical, output_shape)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "# Convert bounding box labels to binary labels (1 for polluted, 0 for not polluted)\n",
    "def convert_to_binary_labels(labels):\n",
    "    binary_labels = [1 if len(label) > 0 else 0 for label in labels]\n",
    "    return binary_labels\n",
    "\n",
    "# Convert train, validation, and test labels\n",
    "train_labels = convert_to_binary_labels(train_labels)\n",
    "valid_labels = convert_to_binary_labels(valid_labels)\n",
    "test_labels = convert_to_binary_labels(test_labels)\n",
    "\n",
    "# Determine the number of classes\n",
    "num_classes = len(set(train_labels))\n",
    "\n",
    "# Convert labels to categorical for classification\n",
    "train_labels = to_categorical(train_labels, num_classes)\n",
    "valid_labels = to_categorical(valid_labels, num_classes)\n",
    "test_labels = to_categorical(test_labels, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid image file: /Users/omkarthorve/Documents/Ganga Flow/Dataset/valid/.DS_Store\n",
      "Skipping invalid image file: /Users/omkarthorve/Documents/Ganga Flow/Dataset/valid/images\n",
      "Skipping invalid image file: /Users/omkarthorve/Documents/Ganga Flow/Dataset/valid/labels\n",
      "Skipping invalid image file: /Users/omkarthorve/Documents/Ganga Flow/Dataset/test/.DS_Store\n",
      "Skipping invalid image file: /Users/omkarthorve/Documents/Ganga Flow/Dataset/test/images\n",
      "Skipping invalid image file: /Users/omkarthorve/Documents/Ganga Flow/Dataset/test/labels\n",
      "Skipping invalid image file: /Users/omkarthorve/Documents/Ganga Flow/Dataset/train/.DS_Store\n",
      "Skipping invalid image file: /Users/omkarthorve/Documents/Ganga Flow/Dataset/train/images\n",
      "Skipping invalid image file: /Users/omkarthorve/Documents/Ganga Flow/Dataset/train/labels\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     69\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/omkarthorve/Documents/Ganga Flow/Dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the actual path\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m analyze_image_dataset(dataset_path)\n",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m, in \u001b[0;36manalyze_image_dataset\u001b[0;34m(dataset_path)\u001b[0m\n\u001b[1;32m     39\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Image Size Distribution\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m widths, heights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mimage_sizes)\n\u001b[1;32m     44\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     45\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "# prompt: I have a Image dataset which has test.train,val folders. write a code to get statistical analysis of that dataset. FYI the image is of ganga river for pollution detection. Also plot data distribution using visualization.\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "def analyze_image_dataset(dataset_path):\n",
    "    \"\"\"\n",
    "    Analyzes an image dataset and plots data distribution.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: Path to the image dataset directory.\n",
    "    \"\"\"\n",
    "    class_counts = {}\n",
    "    image_sizes = []\n",
    "    image_formats = {}\n",
    "\n",
    "    for class_name in os.listdir(dataset_path):\n",
    "        class_dir = os.path.join(dataset_path, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            class_counts[class_name] = 0\n",
    "            for filename in os.listdir(class_dir):\n",
    "                image_path = os.path.join(class_dir, filename)\n",
    "                try:\n",
    "                    with Image.open(image_path) as img:\n",
    "                        image_sizes.append(img.size)\n",
    "                        image_formats[img.format] = image_formats.get(img.format, 0) + 1\n",
    "                        class_counts[class_name] += 1\n",
    "                except IOError:\n",
    "                    print(f\"Skipping invalid image file: {image_path}\")\n",
    "    \n",
    "    # Data Distribution Plots\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()))\n",
    "    plt.title(\"Class Distribution\")\n",
    "    plt.xlabel(\"Classes\")\n",
    "    plt.ylabel(\"Number of Images\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Image Size Distribution\n",
    "    widths, heights = zip(*image_sizes)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(widths, bins=20)\n",
    "    plt.title(\"Width Distribution\")\n",
    "    plt.xlabel(\"Width\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(heights, bins=20)\n",
    "    plt.title(\"Height Distribution\")\n",
    "    plt.xlabel(\"Height\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    # Image Format Distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.pie(image_formats.values(), labels=image_formats.keys(), autopct='%1.1f%%')\n",
    "    plt.title(\"Image Format Distribution\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Class Counts:\", class_counts)\n",
    "    print(\"Image Formats:\", image_formats)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "dataset_path = \"/Users/omkarthorve/Documents/Ganga Flow/Dataset\"  # Replace with the actual path\n",
    "analyze_image_dataset(dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
